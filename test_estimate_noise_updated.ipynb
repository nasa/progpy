{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise Estimation Brute Force Approach\n",
    "\n",
    "- For a single noise value, have multiple simulations for that and that will show the distribution of possible values at that noise level\n",
    "-  Note that each noise value samples from a distribution already, and the same noise value is going to have different results each time due to this\n",
    "- References: MonteCarlo from monte_carlo.py and example in 2024PHMTutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from progpy.models import ThrownObject\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from progpy.predictors import MonteCarlo\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Pick a process and measurement noise as a goal --> then use that as \"real\" data\n",
    "PROCESS_NOISE = 1.8\n",
    "MEASUREMENT_NOISE = 0.4\n",
    "\n",
    "m = ThrownObject(process_noise=PROCESS_NOISE, measurement_noise=MEASUREMENT_NOISE)\n",
    "initial_state = m.initialize()\n",
    "\n",
    "simulated_results = m.simulate_to(8, save_freq=1)\n",
    "\n",
    "times = simulated_results.times\n",
    "inputs = simulated_results.inputs\n",
    "outputs = simulated_results.outputs\n",
    "\n",
    "def future_loading(t, x=None):\n",
    "    return {}\n",
    "\n",
    "STEP_SIZE = 1\n",
    "NUM_SAMPLES = 100\n",
    "PREDICTION_HORIZON = times[-1]\n",
    "SAVE_FREQ = 1\n",
    "\n",
    "og_mc = MonteCarlo(m)\n",
    "og_mc_results = og_mc.predict(initial_state, future_loading, dt=STEP_SIZE, horizon=PREDICTION_HORIZON, save_freq=SAVE_FREQ, n_samples=NUM_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROCESS_NOISE_GUESS = 3\n",
    "MEASUREMENT_NOISE_GUESS = 2\n",
    "\n",
    "m = ThrownObject(process_noise=PROCESS_NOISE_GUESS, measurement_noise=MEASUREMENT_NOISE_GUESS)\n",
    "initial_state = m.initialize()\n",
    "\n",
    "mc = MonteCarlo(m)\n",
    "mc_results = mc.predict(initial_state, future_loading, dt=STEP_SIZE, horizon=PREDICTION_HORIZON, save_freq=SAVE_FREQ, n_samples=NUM_SAMPLES)\n",
    "mc_results.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot MC guess results\n",
    "for i in range(NUM_SAMPLES):\n",
    "    data_pos = [state['x'] for state in mc_results.outputs[i]]\n",
    "    plt.plot(mc_results.times[:len(data_pos)], data_pos, color=\"orange\", alpha=0.2)\n",
    "\n",
    "# Plot real data results\n",
    "for i in range(NUM_SAMPLES):\n",
    "    data_pos = [state['x'] for state in og_mc_results.outputs[i]]\n",
    "    plt.plot(og_mc_results.times[:len(data_pos)], data_pos, color=\"teal\", alpha=0.2)\n",
    "\n",
    "# Plot model with no noise\n",
    "m = ThrownObject(process_noise=0, measurement_noise=0)\n",
    "simulated_results = m.simulate_to(times[-1], save_freq=1)\n",
    "no_noise_pos = [state['x'] for state in simulated_results.outputs]\n",
    "plt.plot(times, no_noise_pos, color=\"black\", linestyle=\"--\", label=\"No noise simulation\")\n",
    "\n",
    "# Plot real data mean\n",
    "og_mean_pos = [state['x'] for state in og_mc_results.outputs.mean]\n",
    "plt.plot(times, og_mean_pos, color=\"blue\", linestyle=\"--\", label=\"Real data mean\")\n",
    "\n",
    "# Plot MC guess mean\n",
    "mc_mean_pos = [state['x'] for state in mc_results.outputs.mean]\n",
    "plt.plot(times, mc_mean_pos, color=\"tab:orange\", linestyle=\"--\", label=\"MC guess mean\")\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"Monte Carlo simulation for one noise value of Thrown Object\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Position (m)\")\n",
    "plt.show()\n",
    "\n",
    "error = np.mean((np.array(og_mean_pos) - np.array(mc_mean_pos))**2)\n",
    "print(\"MSE error between real data and MC guess mean: \", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distrbutions from Brute Force Approach\n",
    "- At one time step (like the last one of 8 or let's say 6), get the distribution and compare to the data (which is also a distribution and compare that - get the error value between them and use that to optimize)\n",
    "- With other noise values (like 5 or 10% below or above optimized number), see how sensitivity changes (mean and variance) after the optimization\n",
    "   - Helps user understand how precise they need to be with noise number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(9):\n",
    "    dist = []\n",
    "    for i in mc_results.outputs:\n",
    "        if len(i) > x:\n",
    "            dist.append(i[x]['x'])\n",
    "\n",
    "    og_dist = []\n",
    "    for i in og_mc_results.outputs:\n",
    "        if len(i) > x:\n",
    "            og_dist.append(i[x]['x'])\n",
    "\n",
    "    print(\"MC guess distribution mean: \", np.mean(dist))\n",
    "    print(\"MC guess distribution std: \", np.std(dist))\n",
    "\n",
    "    print(\"\\nReal data distribution mean: \", np.mean(og_dist))\n",
    "    print(\"Real data distribution std: \", np.std(og_dist))\n",
    "\n",
    "    print(\"\\nReal data - MC guess difference mean: \", np.mean(og_dist) - np.mean(dist))\n",
    "    print(\"Real data - MC guess difference std: \", np.std(og_dist) - np.std(dist))\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "    axes[0].hist(og_dist, color=\"teal\")\n",
    "    axes[1].hist(dist, color=\"tab:orange\")\n",
    "\n",
    "    axes[0].set_title(\"Real data\")\n",
    "    axes[0].set_xlabel(\"Position (m)\")\n",
    "    axes[0].set_ylabel(\"Count\")\n",
    "\n",
    "    axes[1].set_title(\"MC guess\")\n",
    "    axes[1].set_xlabel(\"Position (m)\")\n",
    "    axes[1].set_ylabel(\"Count\")\n",
    "\n",
    "    fig.suptitle(\"Distribution at time \" + str(x))\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real data\n",
    "[state['x'] for state in og_mc_results.outputs.mean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MC guess\n",
    "[state['x'] for state in mc_results.outputs.mean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimize the error between the simulated data and the original data\n",
    "\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "original_data = [state['x'] for state in og_mc_results.outputs.mean]\n",
    "\n",
    "def monte_carlo_simulation(process_noise, measurement_noise):\n",
    "    m = ThrownObject(process_noise=process_noise, measurement_noise=measurement_noise)\n",
    "    initial_state = m.initialize()\n",
    "\n",
    "    mc = MonteCarlo(m)\n",
    "    mc_results = mc.predict(initial_state, future_loading, dt=STEP_SIZE, horizon=PREDICTION_HORIZON, save_freq=SAVE_FREQ, n_samples=NUM_SAMPLES)\n",
    "    \n",
    "    simulated_data = [state['x'] for state in mc_results.outputs.mean]\n",
    "    \n",
    "    return simulated_data\n",
    "\n",
    "def optimization_fcn(params):\n",
    "    process_noise = params[0]\n",
    "    measurement_noise = params[1]\n",
    "    \n",
    "    simulated_data = monte_carlo_simulation(process_noise, measurement_noise)\n",
    "    \n",
    "    # Calculate the MSQ error between simulated and original data (mean and std deviation)\n",
    "    mean_error = (np.mean(simulated_data) - np.mean(original_data))**2\n",
    "    std_error = (np.std(simulated_data) - np.std(original_data))**2\n",
    "    \n",
    "    total_error = mean_error + std_error\n",
    "\n",
    "    print(\"Process Noise: \", process_noise)\n",
    "    print(\"Measurement Noise: \", measurement_noise)\n",
    "    print(\"Total Error: \", total_error, \"\\n\")\n",
    "    \n",
    "    return total_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual is [1.8, 0.4]\n",
    "initial_guess = [3, 2]\n",
    "\n",
    "# Run the optimization using scipy minimize\n",
    "result = minimize(optimization_fcn, initial_guess, method='Nelder-Mead', bounds=((0, 5), (0, 5)))\n",
    "\n",
    "# Display the optimized process and measurement noise\n",
    "print(\"\\nOptimized process noise:\", result.x[0])\n",
    "print(\"Optimized measurement noise:\", result.x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual is [1.8, 0.4]\n",
    "initial_guess = [3, 2]\n",
    "\n",
    "# Run the optimization using scipy minimize\n",
    "result = minimize(optimization_fcn, initial_guess, method='Powell', bounds=((0, 5), (0, 5)))\n",
    "\n",
    "# Display the optimized process and measurement noise\n",
    "print(\"\\nOptimized process noise:\", result.x[0])\n",
    "print(\"Optimized measurement noise:\", result.x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Could possibly specify the frequency at which we compare (like a eval freq) instead of doing at every time step (could also investigate\n",
    "# coarse to fine optimization section)\n",
    "\n",
    "# Look at other measures of distributional similarity - see link in chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coarse to fine optimization\n",
    "- Running a few samples (either running 15 mc simulations or running to time 2 seconds)\n",
    "- Measure runtime, computational power, accuracy of results (since we know our actual noise values since we made the data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore other optimization methods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
